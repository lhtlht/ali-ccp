{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "from tqdm import tqdm\n",
    "from common_val import *\n",
    "from common_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 公共函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_list_split(x):\n",
    "    feature_dict = defaultdict(list)\n",
    "    for fea in x.split('\\x01'):\n",
    "        field = re.split('\\x02', fea)\n",
    "        #fea = re.split('\\x03', field[1])\n",
    "        #feature_dict[field[0]].append({'feature_id':fea[0], 'value':fea[1]})\n",
    "        feature_dict[field[0]].append(field[1])\n",
    "    return feature_dict\n",
    "\n",
    "def mul_list_to_dict(x):\n",
    "    feature_dict = dict()\n",
    "    for item in x:\n",
    "        item = item.split('\\x03')\n",
    "        \n",
    "        feature_dict[item[0]] = item[1]\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = ['101','121','122','124','125','126','127', '128', '129', '150_14', '127_14', '109_14', '110_14']\n",
    "item_features = ['205','206','207', '210','216','508','509', '702', '853', '301']\n",
    "\n",
    "fea_cols = ['common_feature_index', 'feature_num2', 'feature_list2']\n",
    "sample_cols = ['sample_id', 'click', 'conversion', 'common_feature_index', 'feature_num1', 'feature_list1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 公共特征预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_fea_fn(data):\n",
    "    data['fea_dict2'] =  data['feature_list2'].map(feature_list_split)\n",
    "    data['userid'] = data['fea_dict2'].map(lambda x: int(x['101'][0].split('\\x03')[0]) if '101' in x else 0) # 用户ID\n",
    "    data['usercate1'] = data['fea_dict2'].map(lambda x: int(x['121'][0].split('\\x03')[0]) if '121' in x else 0) # 用户的一种分类ID\n",
    "    data['usercate2'] = data['fea_dict2'].map(lambda x: int(x['122'][0].split('\\x03')[0]) if '122' in x else 0) # 用户的一种分类ID\n",
    "    data['usercate1'] = data['usercate1'].map(lambda x: x-3438658+1 if x!=0 else x)\n",
    "    data['usercate2'] = data['usercate2'].map(lambda x: x-3438755+1 if x!=0 else x)\n",
    "    \n",
    "    data['gender'] = data['fea_dict2'].map(lambda x: int(x['124'][0].split('\\x03')[0]) if '124' in x else 0) # 用户性别分类ID\n",
    "    data['gender'] = data['gender'].map({3438769:2, 3438768:1, 0:0})\n",
    "\n",
    "    data['age'] = data['fea_dict2'].map(lambda x: int(x['125'][0].split('\\x03')[0]) if '125' in x else 0) # 用户年龄分类ID\n",
    "    data['age'] = data['age'].map(lambda x: x-3438770+1 if x!=0 else x)\n",
    "    \n",
    "    data['user_consume1'] = data['fea_dict2'].map(lambda x: int(x['126'][0].split('\\x03')[0]) if '126' in x else 0) # 用户消费水平分类I    \n",
    "    data['user_consume2'] = data['fea_dict2'].map(lambda x: int(x['127'][0].split('\\x03')[0]) if '127' in x else 0) # 用户消费水平分类II\n",
    "    data['user_consume1'] = data['user_consume1'].map({3438777:1, 3438778:2, 3438779:3, 0:0})\n",
    "    data['user_consume2'] = data['user_consume2'].map({3438780:1, 3438781:2, 3438782:3, 0:0})\n",
    "    \n",
    "    data['work'] = data['fea_dict2'].map(lambda x: int(x['128'][0].split('\\x03')[0]) if '128' in x else 0) # 用户是否就业\n",
    "    data['work'] = data['work'].map({3864885:1, 3864886:2, 0:0})\n",
    "    \n",
    "    data['location'] = data['fea_dict2'].map(lambda x: int(x['129'][0].split('\\x03')[0]) if '129' in x else 0) # 用户地理信息分类ID\n",
    "    data['location'] = data['location'].map({3864887:1, 3864888:2, 3864889:3, 3864890:4, 0:0})\n",
    "    \n",
    "#     data['user_intention_node_count'] = data['fea_dict2'].map(lambda x: mul_list_to_dict(x['150_14']) if '150_14' in x else {}) # 用户意图ID以及用户在该意图上的历史行为累积数量\n",
    "#     data['user_shop_brand_count'] = data['fea_dict2'].map(lambda x: mul_list_to_dict(x['127_14']) if '127_14' in x else {}) # 商品品牌ID以及用户在该店铺上的历史行为累积数量*\n",
    "#     data['user_shop_cate_count'] = data['fea_dict2'].map(lambda x: mul_list_to_dict(x['109_14']) if '109_14' in x else {}) # 商品类目ID以及用户在该类目上的历史行为累积数量*\n",
    "#     data['user_shop_count'] = data['fea_dict2'].map(lambda x: mul_list_to_dict(x['110_14']) if '110_14' in x else {}) # 商品店铺ID以及用户在该店铺上的历史行为累积数量*\n",
    "    \n",
    "    data['user_intention_count'] = data['fea_dict2'].map(lambda x: x['150_14']) # 用户意图ID以及用户在该意图上的历史行为累积数量\n",
    "    data['user_brand_count'] = data['fea_dict2'].map(lambda x: x['127_14']) # 商品品牌ID以及用户在该店铺上的历史行为累积数量*\n",
    "    data['user_cate_count'] = data['fea_dict2'].map(lambda x: x['109_14']) # 商品类目ID以及用户在该类目上的历史行为累积数量*\n",
    "    data['user_shop_count'] = data['fea_dict2'].map(lambda x: x['110_14']) # 商品店铺ID以及用户在该店铺上的历史行为累积数量*\n",
    "    \n",
    "    # 后续字段类型转换\n",
    "    data['usercate1'] = data['usercate1'].astype('category')\n",
    "    data['usercate2'] = data['usercate2'].astype('category')\n",
    "    data['gender'] = data['gender'].astype('category')\n",
    "    data['age'] = data['age'].astype('category')\n",
    "    data['user_consume1'] = data['user_consume1'].astype('category')\n",
    "    data['user_consume2'] = data['user_consume2'].astype('category')\n",
    "    data['work'] = data['work'].astype('category')\n",
    "    data['location'] = data['location'].astype('category')\n",
    "    data.drop(columns=['feature_num2','feature_list2', 'fea_dict2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  内存大，一次性读入处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read done'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'done!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5da2395c3def>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0muser_fea_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfea_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;34m'done!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfea_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'fea_train.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(self, path, compression, protocol)\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2676\u001b[1;33m         \u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2678\u001b[0m     def to_clipboard(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(obj, filepath_or_buffer, compression, protocol)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fea_train = pd.read_csv(common_features_train_csv, header=None, names=fea_cols)\n",
    "'read done'\n",
    "user_fea_fn(fea_train)\n",
    "'done!'\n",
    "fea_train.to_pickle(os.path.join(data_path2, f'fea_train.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read done'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'done!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_test = pd.read_csv(common_features_test_csv, header=None, names=fea_cols)\n",
    "'read done'\n",
    "user_fea_fn(fea_test)\n",
    "'done!'\n",
    "fea_test.to_pickle(os.path.join(data_path2, f'fea_test.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  内存小，一次性读入处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train = pd.read_csv(common_features_train_csv, header=None, names=fea_cols, iterator=True,chunksize = 100000)\n",
    "fea_test = pd.read_csv(common_features_test_csv, header=None, names=fea_cols, iterator=True,chunksize = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (100000, 3)\n",
      "1 (100000, 3)\n",
      "2 (100000, 3)\n",
      "3 (100000, 3)\n",
      "4 (100000, 3)\n",
      "5 (100000, 3)\n",
      "6 (100000, 3)\n",
      "7 (30600, 3)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for chunk_df in fea_train:\n",
    "    print(i,chunk_df.shape)\n",
    "    user_fea_fn(chunk_df)\n",
    "    chunk_df.to_pickle(os.path.join(data_path2, f'fea_train{i}.pkl'))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (100000, 3)\n",
      "1 (100000, 3)\n",
      "2 (100000, 3)\n",
      "3 (100000, 3)\n",
      "4 (100000, 3)\n",
      "5 (100000, 3)\n",
      "6 (100000, 3)\n",
      "7 (100000, 3)\n",
      "8 (84212, 3)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for chunk_df in fea_test:\n",
    "    print(i,chunk_df.shape)\n",
    "    user_fea_fn(chunk_df)\n",
    "    chunk_df.to_pickle(os.path.join(data_path2, f'fea_test{i}.pkl'))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样本特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = pd.read_csv(sample_skeleton_train_csv, header=None, names=sample_cols, iterator=True,chunksize = 2500000)\n",
    "sample_test = pd.read_csv(sample_skeleton_test_csv, header=None, names=sample_cols, iterator=True,chunksize = 2500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def item_fea_fn(data):\n",
    "    data['fea_dict1'] =  data['feature_list1'].map(feature_list_split)\n",
    "    \n",
    "    data['itemid'] = data['fea_dict1'].map(lambda x: int(x['205'][0].split('\\x03')[0]) if '205' in x else 0) # 商品ID\n",
    "    data['item_cate'] = data['fea_dict1'].map(lambda x: int(x['206'][0].split('\\x03')[0]) if '206' in x else 0) # 商品所属类目ID\n",
    "    data['shopid'] = data['fea_dict1'].map(lambda x: int(x['207'][0].split('\\x03')[0]) if '207' in x else 0) # 商品所属店铺ID\n",
    "    data['brandid'] = data['fea_dict1'].map(lambda x: int(x['216'][0].split('\\x03')[0]) if '216' in x else 0) # 商品的品牌ID\n",
    "    data['business'] = data['fea_dict1'].map(lambda x: int(x['301'][0].split('\\x03')[0]) if '301' in x else 0) # 业务场景信息的一种分类表示\n",
    "    \n",
    "    # 109_14:商品类目ID以及用户在该类目上的历史行为累积数量*和206域商品所属类目IDe的组合特征：浮点值 商品所属类目ID 经验证，一一对应关系\n",
    "    data['user_cate_val'] = data['fea_dict1'].map(lambda x:x['508'][0].split('\\x03')[1] if len(x['508'])>0 else np.NaN) \n",
    "    # 110_14和207域的组合特征：浮点值,经验证，一一对应关系\n",
    "    data['user_shop_val'] = data['fea_dict1'].map(lambda x:x['509'][0].split('\\x03')[1] if len(x['509'])>0 else np.NaN) \n",
    "    # 127_14和216域的组合特征：浮点值 经验证，一一对应关系\n",
    "    data['user_brand_val'] = data['fea_dict1'].map(lambda x:x['702'][0].split('\\x03')[1] if len(x['702'])>0 else np.NaN) \n",
    "    \n",
    "    data['user_intentions'] = data['fea_dict1'].map(lambda x:[i.split('\\x03')[0] for i in x['210']]) # 商品关联用户意图ID：多值\n",
    "    data['user_intentions_val'] = data['fea_dict1'].map(lambda x:x['853']) # 150_14和210域的组合特征：多值，浮点值\n",
    "    data.drop(columns=['feature_num1','feature_list1', 'fea_dict1'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_fea_train = pd.read_pickle(os.path.join(data_path2, f'fea_train.pkl'))\n",
    "common_fea_train = pd.concat([pd.read_pickle(os.path.join(data_path2, f'fea_train{i}.pkl')) for i in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (2500000, 6)\n",
      "2 (2500000, 6)\n",
      "3 (2500000, 6)\n",
      "4 (2500000, 6)\n",
      "5 (2500000, 6)\n",
      "6 (2500000, 6)\n",
      "7 (2500000, 6)\n",
      "8 (2500000, 6)\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for sam_chunk in sample_train:\n",
    "    print(i,sam_chunk.shape)\n",
    "    item_fea_fn(sam_chunk)\n",
    "    \n",
    "    sam_chunk = sam_chunk.merge(common_fea_train, how='left', on='common_feature_index')\n",
    "    sam_chunk.to_pickle(os.path.join(data_path2, f'sample_train{i}.pkl'))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(data_path2, f'sample_train1.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(os.path.join(data_path2, f'sample_train1.pkl'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIO\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mAnyStr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprotocol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "\u001b[1;32mdef\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFilePathOrBuffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"infer\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprotocol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"\n",
       "    Pickle (serialize) object to file.\n",
       "\n",
       "    Parameters\n",
       "    ----------\n",
       "    obj : any object\n",
       "        Any python object.\n",
       "    filepath_or_buffer : str, path object or file-like object\n",
       "        File path, URL, or buffer where the pickled object will be stored.\n",
       "\n",
       "        .. versionchanged:: 1.0.0\n",
       "           Accept URL. URL has to be of S3 or GCS.\n",
       "\n",
       "    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
       "        If 'infer' and 'path_or_url' is path-like, then detect compression from\n",
       "        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n",
       "        compression) If 'infer' and 'path_or_url' is not path-like, then use\n",
       "        None (= no decompression).\n",
       "    protocol : int\n",
       "        Int which indicates which protocol should be used by the pickler,\n",
       "        default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible\n",
       "        values for this parameter depend on the version of Python. For Python\n",
       "        2.x, possible values are 0, 1, 2. For Python>=3.0, 3 is a valid value.\n",
       "        For Python >= 3.4, 4 is a valid value. A negative value for the\n",
       "        protocol parameter is equivalent to setting its value to\n",
       "        HIGHEST_PROTOCOL.\n",
       "\n",
       "        .. [1] https://docs.python.org/3/library/pickle.html\n",
       "\n",
       "    See Also\n",
       "    --------\n",
       "    read_pickle : Load pickled pandas object (or any object) from file.\n",
       "    DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n",
       "    DataFrame.to_sql : Write DataFrame to a SQL database.\n",
       "    DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n",
       "\n",
       "    Examples\n",
       "    --------\n",
       "    >>> original_df = pd.DataFrame({\"foo\": range(5), \"bar\": range(5, 10)})\n",
       "    >>> original_df\n",
       "       foo  bar\n",
       "    0    0    5\n",
       "    1    1    6\n",
       "    2    2    7\n",
       "    3    3    8\n",
       "    4    4    9\n",
       "    >>> pd.to_pickle(original_df, \"./dummy.pkl\")\n",
       "\n",
       "    >>> unpickled_df = pd.read_pickle(\"./dummy.pkl\")\n",
       "    >>> unpickled_df\n",
       "       foo  bar\n",
       "    0    0    5\n",
       "    1    1    6\n",
       "    2    2    7\n",
       "    3    3    8\n",
       "    4    4    9\n",
       "\n",
       "    >>> import os\n",
       "    >>> os.remove(\"./dummy.pkl\")\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_filepath_or_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"infer\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0m_f\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0m_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mfp_or_buf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\haotaolin\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??pd.to_pickle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
